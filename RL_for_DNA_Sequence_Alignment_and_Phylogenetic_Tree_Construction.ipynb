{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sierra-hunt/github-and-kaggle-ML-work/blob/main/RL_for_DNA_Sequence_Alignment_and_Phylogenetic_Tree_Construction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phylogenetic tree building whilst aligning muptiple sequences through Reinforcement Learning\n",
        "\n",
        "This research prototype implements a reinforcement learning framework for simultaneously optimizing DNA sequence alignment and phylogenetic tree construction. The system uses DNABERT-2 transformer embeddings to represent genetic sequences, then applies Monte Carlo Tree Search (MCTS) to explore alignment modifications and tree adjustments. The approach considers multiple RL strategies (Q-Learning, Policy Gradient, PPO) for this dual-optimization problem, balancing alignment quality with phylogenetic consistency. The implementation includes parallel simulation, Newick tree parsing, and a complete RL environment for bioinformatics optimization.\n",
        "\n",
        "**This wasn't for my university at all, and is just something I was working on in my spare time as a possible extension of the work I did for my Masters. This project demonstrates a *designed but not fully implemented* reinforcement learning system for phylogenetic optimization.**\n"
      ],
      "metadata": {
        "id": "NbA9V7_fluoe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBrZUSzUrUh"
      },
      "source": [
        "Rough look into the reasoning for my approach here:\n",
        "\n",
        "Q-Learning / DQN\n",
        "- Discrete action space, simple alignment modifications\n",
        "- Efficient, works well with small to medium-sized datasets\n",
        "- May struggle with large sequence states; requires dimensionality reduction\n",
        "\n",
        "Policy Gradient Methods\n",
        "- Continuous or complex action space (e.g., fine-tuning alignments)\n",
        "- Can model complex actions, better for interdependent decisions\n",
        "- Sample inefficient, requires fine-tuning for stability\n",
        "\n",
        "Proximal Policy Optimization (PPO)\n",
        "- Complex, continuous, interdependent actions, stability-focused\n",
        "- Stable, sample-efficient, robust learning\n",
        "- Computationally expensive, requires careful tuning\n",
        "\n",
        "MCTS\n",
        "- State representation is an explicit search tree representation of actions (alignment changes or tree modifications)\n",
        "- Does not require a predefined reward function or policy, making it easier to adapt to unknown or complex problem domains.\n",
        "-  Stable as it relies on incremental backpropagation of rewards\n",
        "- The effectiveness of MCTS depends heavily on the quality of the simulations (playouts).\n",
        "- Computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHLoOCj4UjOQ",
        "outputId": "a715e366-9a28-42a5-daa9-57964f11871c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1930 to 1932\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1932 to 1952\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1952 to 1961\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1961 to 1982\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1982 to 1987\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1987 to 1996\n",
            "  warnings.warn(\n",
            "/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 1996 to 2018\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers.models.bert.configuration_bert import BertConfig\n",
        "import json\n",
        "import os\n",
        "import csv  # Ensure to import csv for reading the file\n",
        "\n",
        "# Load the DNABERT-2 tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
        "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
        "\n",
        "# Define file paths\n",
        "csv_file = r\"/content/drive/MyDrive/2024-25 RL/Prototype/simulations_data_unaligned_MSA_and_trees.csv\"\n",
        "embedding_dir = r\"/content/drive/MyDrive/2024-25 RL/Prototype/embeddings\"\n",
        "os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "def tokenize_and_embed(sequence, kmer_size=6):\n",
        "    \"\"\"Tokenizes and embeds a DNA sequence with both mean and max pooling.\"\"\"\n",
        "    # Split sequence into kmers\n",
        "    kmer_tokens = [sequence[i:i+kmer_size] for i in range(len(sequence) - kmer_size + 1)]\n",
        "    # Tokenize the kmers\n",
        "    encoded_input = tokenizer(\" \".join(kmer_tokens), return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Get model output and compute the embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_input)\n",
        "\n",
        "        # If the model returns a tuple, the first element contains the hidden states\n",
        "        hidden_states = outputs[0]  # Shape: [1, sequence_length, 768]\n",
        "\n",
        "        # Mean pooling: Average hidden states across the sequence length dimension\n",
        "        embedding_mean = torch.mean(hidden_states[0], dim=0).squeeze().numpy()\n",
        "\n",
        "        # Max pooling: Take the max hidden state across the sequence length dimension\n",
        "        embedding_max = torch.max(hidden_states[0], dim=0)[0].squeeze().numpy()\n",
        "\n",
        "    # Convert NumPy arrays to lists for JSON serialization\n",
        "    return embedding_mean.tolist(), embedding_max.tolist()\n",
        "\n",
        "# Open and read the CSV file\n",
        "with open(csv_file, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        sim_id = row[\"Simulation\"]\n",
        "        sim_data = json.loads(row[\"Data (JSON)\"])\n",
        "        unaligned_sequences = sim_data[\"unaligned_sequences\"]\n",
        "\n",
        "        embeddings_mean = {}\n",
        "        embeddings_max = {}\n",
        "\n",
        "        # Process each sequence and compute embeddings\n",
        "        for seq_id, seq in unaligned_sequences.items():\n",
        "            embedding_mean, embedding_max = tokenize_and_embed(seq)\n",
        "            embeddings_mean[seq_id] = embedding_mean\n",
        "            embeddings_max[seq_id] = embedding_max\n",
        "\n",
        "        # Save embeddings to JSON files\n",
        "        output_mean_file = os.path.join(embedding_dir, f\"{sim_id}_mean_embeddings.json\")\n",
        "        output_max_file = os.path.join(embedding_dir, f\"{sim_id}_max_embeddings.json\")\n",
        "\n",
        "        # Save embeddings for mean pooling\n",
        "        with open(output_mean_file, 'w') as emb_file_mean:\n",
        "            json.dump(embeddings_mean, emb_file_mean)\n",
        "\n",
        "        # Save embeddings for max pooling\n",
        "        with open(output_max_file, 'w') as emb_file_max:\n",
        "            json.dump(embeddings_max, emb_file_max)\n",
        "\n",
        "print(f\"Embeddings saved in: {embedding_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k3T4MJN8ClB"
      },
      "outputs": [],
      "source": [
        "# Representing the Phylogenetic Tree as an Adjacency Matrix\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def parse_newick_tree(newick):\n",
        "    \"\"\"Parses a Newick string into a tree dictionary.\"\"\"\n",
        "    from ete3 import Tree\n",
        "    t = Tree(newick, format=1)\n",
        "    nodes = list(t.traverse())\n",
        "    node_map = {node.name: idx for idx, node in enumerate(nodes)}\n",
        "    adjacency_matrix = np.zeros((len(nodes), len(nodes)))\n",
        "\n",
        "    for node in nodes:\n",
        "        if not node.is_leaf():\n",
        "            for child in node.children:\n",
        "                adjacency_matrix[node_map[node.name], node_map[child.name]] = child.dist\n",
        "                adjacency_matrix[node_map[child.name], node_map[node.name]] = child.dist\n",
        "\n",
        "    return adjacency_matrix, node_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12pHIZfk8SJp"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None, action=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0.0\n",
        "        self.action = action  # Store the action that led to this node\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        \"\"\"Check if the node has children (fully expanded).\"\"\"\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def best_child(self, exploration_weight=1.0):\n",
        "        \"\"\"Select the best child based on value and exploration.\"\"\"\n",
        "        scores = [\n",
        "            (child.value / (child.visits + 1e-6)) +\n",
        "            exploration_weight * np.sqrt(np.log(self.visits + 1) / (child.visits + 1e-6))\n",
        "            for child in self.children\n",
        "        ]\n",
        "        return self.children[np.argmax(scores)]\n",
        "\n",
        "\n",
        "class MCTSState:\n",
        "    def __init__(self, embeddings, tree_adj_matrix, tree_node_mapping, alignment_score=0.0, tree_score=0.0):\n",
        "        self.embeddings = embeddings\n",
        "        self.tree_adj_matrix = tree_adj_matrix\n",
        "        self.tree_node_mapping = tree_node_mapping\n",
        "        self.alignment_score = alignment_score\n",
        "        self.tree_score = tree_score\n",
        "\n",
        "    def get_combined_score(self, alpha=0.5):\n",
        "        \"\"\"Combines alignment and tree scores.\"\"\"\n",
        "        return alpha * self.alignment_score + (1 - alpha) * self.tree_score\n",
        "\n",
        "\n",
        "def initialize_state(simulation_id, embedding_dir, newick_tree):\n",
        "    \"\"\"Initialize MCTS state from embedding and tree data.\"\"\"\n",
        "    embedding_file = f\"{embedding_dir}/{simulation_id}_embeddings.json\"\n",
        "    with open(embedding_file, 'r') as file:\n",
        "        embeddings = json.load(file)\n",
        "\n",
        "    tree_adj_matrix, tree_node_mapping = parse_newick_tree(newick_tree)\n",
        "\n",
        "    alignment_score = compute_alignment_score(embeddings)  # Replace with real scoring function\n",
        "    tree_score = compute_tree_score(tree_adj_matrix)       # Replace with real scoring function\n",
        "\n",
        "    return MCTSState(\n",
        "        embeddings=embeddings,\n",
        "        tree_adj_matrix=tree_adj_matrix,\n",
        "        tree_node_mapping=tree_node_mapping,\n",
        "        alignment_score=alignment_score,\n",
        "        tree_score=tree_score\n",
        "    )\n",
        "\n",
        "\n",
        "def simulate(state):\n",
        "    \"\"\"Placeholder for a domain-specific simulation.\"\"\"\n",
        "    alignment_score = np.random.rand()  # Replace with real scoring function\n",
        "    tree_score = np.random.rand()      # Replace with real scoring function\n",
        "    return 0.5 * alignment_score + 0.5 * tree_score\n",
        "\n",
        "\n",
        "def expand(node):\n",
        "    \"\"\"Expand a node by generating children based on alignment and tree actions.\"\"\"\n",
        "    state = node.state\n",
        "    new_states = []\n",
        "\n",
        "    # Generate alignment actions\n",
        "    alignment_actions = generate_alignment_actions(state.embeddings.values())\n",
        "\n",
        "    # For each alignment action, generate corresponding tree actions\n",
        "    for align_action in alignment_actions:\n",
        "        updated_sequences = apply_alignment_action(state.embeddings.values(), align_action)\n",
        "\n",
        "        # Generate tree actions based on the updated alignment\n",
        "        tree_actions = generate_tree_actions(state.tree_adj_matrix, updated_sequences)\n",
        "\n",
        "        for tree_action in tree_actions:\n",
        "            compound_action = {\"alignment\": align_action, \"tree\": tree_action}\n",
        "            new_state = apply_compound_action(state, compound_action)\n",
        "            new_states.append(MCTSNode(state=new_state, parent=node, action=compound_action))\n",
        "\n",
        "    return new_states\n",
        "\n",
        "\n",
        "def apply_compound_action(state, action):\n",
        "    \"\"\"Apply both alignment and tree actions to generate a new state.\"\"\"\n",
        "    # Apply alignment action\n",
        "    alignment_action = action[\"alignment\"]\n",
        "    new_sequences = apply_alignment_action(state.embeddings.values(), alignment_action)\n",
        "\n",
        "    # Update tree based on new alignment\n",
        "    tree_action = action[\"tree\"]\n",
        "    new_tree_adj_matrix = modify_tree_based_on_action(state.tree_adj_matrix, tree_action, new_sequences)\n",
        "\n",
        "    return MCTSState(\n",
        "        embeddings=compute_embeddings(new_sequences),\n",
        "        tree_adj_matrix=new_tree_adj_matrix,\n",
        "        tree_node_mapping=state.tree_node_mapping,\n",
        "        alignment_score=compute_alignment_score(new_sequences),\n",
        "        tree_score=compute_tree_score(new_tree_adj_matrix)\n",
        "    )\n",
        "\n",
        "\n",
        "def insert_gap(sequence, position):\n",
        "    \"\"\"Inserts a gap at a specific position in the sequence.\"\"\"\n",
        "    return sequence[:position] + \"-\" + sequence[position:]\n",
        "\n",
        "\n",
        "def shift_sequence(sequence, shift):\n",
        "    \"\"\"Shifts the sequence by adding gaps to the left or right.\"\"\"\n",
        "    if shift > 0:\n",
        "        return \"-\" * shift + sequence[:-shift]\n",
        "    elif shift < 0:\n",
        "        return sequence[-shift:] + \"-\" * abs(shift)\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def refine_alignment(sequences):\n",
        "    \"\"\"Refines the alignment by minimizing mismatches (placeholder logic).\"\"\"\n",
        "    # Implement a refinement strategy like progressive alignment or a scoring heuristic\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def compute_reward(state, alpha=0.5):\n",
        "    \"\"\"Computes a reward for the state considering alignment and tree scores.\"\"\"\n",
        "    alignment_score = compute_alignment_score(state.embeddings.values())\n",
        "    tree_score = compute_tree_score(state.tree_adj_matrix)\n",
        "    inconsistency_penalty = compute_inconsistency_penalty(state.embeddings, state.tree_adj_matrix)\n",
        "    return alpha * alignment_score + (1 - alpha) * tree_score - inconsistency_penalty\n",
        "\n",
        "\n",
        "def compute_alignment_score(sequences):\n",
        "    \"\"\"Computes a score for the alignment.\"\"\"\n",
        "    score = 0\n",
        "    for i, seq1 in enumerate(sequences):\n",
        "        for j, seq2 in enumerate(sequences):\n",
        "            if i < j:\n",
        "                match_score = sum(1 for a, b in zip(seq1, seq2) if a == b)\n",
        "                gap_penalty = sum(1 for a, b in zip(seq1, seq2) if a == \"-\" or b == \"-\")\n",
        "                score += match_score - 0.5 * gap_penalty  # Example weights\n",
        "    return score / len(sequences)\n",
        "\n",
        "\n",
        "def compute_tree_score(adj_matrix):\n",
        "    \"\"\"Computes a score for the tree structure.\"\"\"\n",
        "    sparsity = np.sum(adj_matrix > 0) / adj_matrix.size  # Fraction of non-zero connections\n",
        "    return 1 - sparsity  # Prefer sparse trees\n",
        "\n",
        "\n",
        "def backpropagate(node, reward):\n",
        "    \"\"\"Propagates the reward up the tree.\"\"\"\n",
        "    while node is not None:\n",
        "        node.value += reward\n",
        "        node.visits += 1\n",
        "        node = node.parent\n",
        "\n",
        "\n",
        "def apply_action(state, action):\n",
        "    \"\"\"Apply a specific action to modify the state.\"\"\"\n",
        "    if action[\"type\"] == \"align\":\n",
        "        new_sequences = refine_alignment(state.embeddings.values(), **action[\"params\"])\n",
        "        new_tree_adj_matrix = update_tree_from_alignment(new_sequences)\n",
        "    elif action[\"type\"] == \"tree\":\n",
        "        new_tree_adj_matrix = modify_tree(state.tree_adj_matrix, **action[\"params\"])\n",
        "        new_sequences = refine_alignment_based_on_tree(state.embeddings.values(), new_tree_adj_matrix)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported action type\")\n",
        "\n",
        "    return MCTSState(\n",
        "        embeddings=compute_embeddings(new_sequences),  # Recompute embeddings\n",
        "        tree_adj_matrix=new_tree_adj_matrix,\n",
        "        tree_node_mapping=state.tree_node_mapping,\n",
        "        alignment_score=compute_alignment_score(new_sequences),\n",
        "        tree_score=compute_tree_score(new_tree_adj_matrix)\n",
        "    )\n",
        "\n",
        "\n",
        "def parallel_mcts(root, simulations=100, parallel_workers=4):\n",
        "    \"\"\"Runs parallel MCTS simulations.\"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=parallel_workers) as executor:\n",
        "        for _ in range(simulations):\n",
        "            node = root\n",
        "            # Selection\n",
        "            while node.is_fully_expanded():\n",
        "                node = node.best_child()\n",
        "\n",
        "            # Expansion\n",
        "            if not node.is_fully_expanded():\n",
        "                expand(node)\n",
        "\n",
        "            # Parallel Rollouts\n",
        "            rewards = list(executor.map(simulate, [child.state for child in node.children]))\n",
        "\n",
        "            # Backpropagation\n",
        "            for child, reward in zip(node.children, rewards):\n",
        "                backpropagate(child, reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAonI1BY8a3p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1emo2MXFRtFcHZ_8ie2b6QrxnAO3CHF3U",
      "authorship_tag": "ABX9TyMsfEYzeq0pTM6XqXi9JK4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}